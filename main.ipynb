{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/01 12:41:47 WARN Utils: Your hostname, fossa-dsa-001 resolves to a loopback address: 127.0.1.1; using 192.168.0.111 instead (on interface wlp3s0)\n",
      "22/07/01 12:41:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/fossa/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/01 12:41:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\"\"\"make an instance of a SparkSession called 'spark'.\"\"\"\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[date: date, open: float, high: float, low: float, close: float, volume: float, currency: string]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our path\n",
    "data_file = 'data/coffee.csv'\n",
    "# set schema\n",
    "customSchema = 'date date, open float, high float, low float, close float, volume float, currency string'\n",
    "# read our csv into a spark df\n",
    "sdf = spark.read.csv(data_file, schema = customSchema, header = True)\n",
    "sdf.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+-----------+----------+--------+----------+\n",
      "|      date|  open|  high|   low| close| volume|currency| open2close|  high2low|did_well|   day_abs|\n",
      "+----------+------+------+------+------+-------+--------+-----------+----------+--------+----------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5| 6640.0|     USD|       5.75|       7.5|    true|      5.75|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25| 5492.0|     USD|        0.0|      4.25|    true|       0.0|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6| 6165.0|     USD| -3.5999985| 2.4000015|    true| 3.5999985|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85| 5094.0|     USD|  2.1500015|  4.550003|    true| 2.1500015|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15| 6855.0|     USD|  3.0999985| 3.5999985|    true| 3.0999985|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55| 7499.0|     USD|   5.949997|  8.449997|    true|  5.949997|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8| 3976.0|     USD|  -2.300003|0.44999695|    true|  2.300003|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95| 5184.0|     USD| -1.1499939|  1.550003|    true| 1.1499939|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55| 3717.0|     USD| 0.69999695|  1.449997|    true|0.69999695|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55|10115.0|     USD|   5.199997|  7.699997|    true|  5.199997|\n",
      "|2000-01-18|111.75|118.25| 110.6|115.75| 7364.0|     USD|       -4.0|       2.5|    true|       4.0|\n",
      "|2000-01-19| 116.5|118.25|114.75| 116.7| 6626.0|     USD|-0.19999695|  1.550003|    true|0.19999695|\n",
      "|2000-01-20|118.25| 118.8| 111.7| 112.0| 8834.0|     USD|       6.25|  6.800003|    true|      6.25|\n",
      "|2000-01-21| 112.0| 113.5| 110.8| 111.2| 5625.0|     USD| 0.80000305|  2.300003|    true|0.80000305|\n",
      "|2000-01-24|110.95| 114.4|110.95| 111.9| 5821.0|     USD| -0.9500046|       2.5|    true| 0.9500046|\n",
      "|2000-01-25| 111.6| 113.7| 111.6|112.85| 4014.0|     USD|      -1.25| 0.8499985|    true|      1.25|\n",
      "|2000-01-26| 112.5| 115.3| 111.9|115.15| 5796.0|     USD| -2.6500015|0.15000153|    true| 2.6500015|\n",
      "|2000-01-27|114.75| 116.4| 112.8| 114.6| 5477.0|     USD| 0.15000153|  1.800003|    true|0.15000153|\n",
      "|2000-01-28| 115.1| 115.4| 113.7| 114.7| 3334.0|     USD| 0.40000153| 0.7000046|    true|0.40000153|\n",
      "|2000-01-31|113.75| 114.0| 110.5| 111.1| 6465.0|     USD|  2.6500015| 2.9000015|    true| 2.6500015|\n",
      "+----------+------+------+------+------+-------+--------+-----------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to the DataFrame where the values are the difference between 'Open' and 'Close'.\n",
    "from pyspark.sql.functions import *\n",
    "sdf = sdf.withColumn('open2close',\n",
    "                (sdf.open - sdf.close))\n",
    "# Add a column to the DataFrame where the values are the difference between 'High' and 'Low'.\n",
    "sdf = sdf.withColumn('high2low',\n",
    "                (sdf.high - sdf.close))\n",
    "# Add a column to the DataFrame where the values are 'True' if the volume for that day was 100 or above, and otherwise 'False'.\n",
    "sdf = sdf.withColumn('did_well',\n",
    "                     when((sdf.volume > 99), lit(True))\n",
    "                     .otherwise(lit(False)))\n",
    "# add another column that contains the absolute values of the numbers in that column.\n",
    "sdf = sdf.withColumn('day_abs', abs(sdf.open2close))\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9827/3191611828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet_sales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m sdf = sdf.withColumn('net_sales',\n\u001b[0;32m----> 3\u001b[0;31m                     col(net_sales))\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# sdf = sdf.withColumn('net_sales',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#                      )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m     \u001b[0mjf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_get_jvm_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m                         \u001b[0mtemp_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/py4j/java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java.util.ArrayList\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mjava_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column is not iterable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;31m# string methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "net_sales = avg(sdf.open + sdf.high + sdf.low + sdf.close) * sdf.volume\n",
    "sdf = sdf.withColumn('net_sales',\n",
    "                    col(net_sales))\n",
    "# sdf = sdf.withColumn('net_sales',\n",
    "#                      )\n",
    "\n",
    "# et_sales which is the average of opening, high, low, and closing cost times the volume: net_sales = avg(opening, high, low, closing price) * volume\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd7cd7597f4635cf9895bddc64f3e7e6e4fea43a0bbbb31f8fc081c6ceb2aeb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
