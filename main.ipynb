{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/01 12:41:47 WARN Utils: Your hostname, fossa-dsa-001 resolves to a loopback address: 127.0.1.1; using 192.168.0.111 instead (on interface wlp3s0)\n",
      "22/07/01 12:41:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/fossa/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/01 12:41:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\"\"\"make an instance of a SparkSession called 'spark'.\"\"\"\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[date: date, open: float, high: float, low: float, close: float, volume: float, currency: string]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our path\n",
    "data_file = 'data/coffee.csv'\n",
    "# set schema\n",
    "customSchema = 'date date, open float, high float, low float, close float, volume float, currency string'\n",
    "# read our csv into a spark df\n",
    "sdf = spark.read.csv(data_file, schema = customSchema, header = True)\n",
    "sdf.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- open2close: float (nullable = true)\n",
      " |-- high2low: float (nullable = true)\n",
      " |-- did_well: boolean (nullable = false)\n",
      " |-- day_abs: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to the DataFrame where the values are the difference between 'Open' and 'Close'.\n",
    "from pyspark.sql.functions import *\n",
    "sdf = sdf.withColumn('open2close',\n",
    "                (sdf.open - sdf.close))\n",
    "# Add a column to the DataFrame where the values are the difference between 'High' and 'Low'.\n",
    "sdf = sdf.withColumn('high2low',\n",
    "                (sdf.high - sdf.close))\n",
    "# Add a column to the DataFrame where the values are 'True' if the volume for that day was 100 or above, and otherwise 'False'.\n",
    "sdf = sdf.withColumn('did_well',\n",
    "                     when((sdf.volume > 99), lit(True))\n",
    "                     .otherwise(lit(False)))\n",
    "# add another column that contains the absolute values of the numbers in that column.\n",
    "sdf = sdf.withColumn('day_abs', abs(sdf.open2close))\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sales = mean(sdf.open + sdf.high + sdf.low + sdf.close) * sdf.volume\n",
    "# print(net_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(day_abs)|\n",
      "+------------------+\n",
      "|1.7606027822995378|\n",
      "+------------------+\n",
      "\n",
      "201\n",
      "+-----------------+\n",
      "|        avg(open)|\n",
      "+-----------------+\n",
      "|126.0496775257701|\n",
      "+-----------------+\n",
      "\n",
      "+---------+\n",
      "|max(high)|\n",
      "+---------+\n",
      "|   306.25|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the average of the values in the column that has the absolute values of the difference between 'Open' and 'Close'.\n",
    "sdf.select(avg(sdf.day_abs)).show()\n",
    "# Get the count of values where the 'Volume' was less than 100.\n",
    "print(sdf.filter(sdf.volume == False).count())\n",
    "# Find the average 'Open' value.\n",
    "sdf.select(avg(sdf.open)).show()\n",
    "# Get the highest 'High' value.\n",
    "sdf.select(max(sdf.high)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf.write.parquet('data/coffequet.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd7cd7597f4635cf9895bddc64f3e7e6e4fea43a0bbbb31f8fc081c6ceb2aeb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
