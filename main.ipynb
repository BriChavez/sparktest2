{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/01 12:41:47 WARN Utils: Your hostname, fossa-dsa-001 resolves to a loopback address: 127.0.1.1; using 192.168.0.111 instead (on interface wlp3s0)\n",
      "22/07/01 12:41:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/fossa/deb/tests/spark_stuff/venv/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/01 12:41:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\"\"\"make an instance of a SparkSession called 'spark'.\"\"\"\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[date: date, open: float, high: float, low: float, close: float, volume: float, currency: string]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our path\n",
    "data_file = 'data/coffee.csv'\n",
    "# set schema\n",
    "customSchema = 'date date, open float, high float, low float, close float, volume float, currency string'\n",
    "# read our csv into a spark df\n",
    "sdf = spark.read.csv(data_file, schema = customSchema, header = True)\n",
    "sdf.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+-------+--------+-----------+----------+--------+----------+\n",
      "|      date|  open|  high|   low| close| volume|currency| open2close|  high2low|did_well|   day_abs|\n",
      "+----------+------+------+------+------+-------+--------+-----------+----------+--------+----------+\n",
      "|2000-01-03|122.25| 124.0| 116.1| 116.5| 6640.0|     USD|       5.75|       7.5|    true|      5.75|\n",
      "|2000-01-04|116.25| 120.5|115.75|116.25| 5492.0|     USD|        0.0|      4.25|    true|       0.0|\n",
      "|2000-01-05| 115.0| 121.0| 115.0| 118.6| 6165.0|     USD| -3.5999985| 2.4000015|    true| 3.5999985|\n",
      "|2000-01-06| 119.0| 121.4| 116.5|116.85| 5094.0|     USD|  2.1500015|  4.550003|    true| 2.1500015|\n",
      "|2000-01-07|117.25|117.75| 113.8|114.15| 6855.0|     USD|  3.0999985| 3.5999985|    true| 3.0999985|\n",
      "|2000-01-10| 123.5| 126.0| 116.7|117.55| 7499.0|     USD|   5.949997|  8.449997|    true|  5.949997|\n",
      "|2000-01-11| 115.5|118.25| 115.5| 117.8| 3976.0|     USD|  -2.300003|0.44999695|    true|  2.300003|\n",
      "|2000-01-12| 117.8| 120.5| 116.9|118.95| 5184.0|     USD| -1.1499939|  1.550003|    true| 1.1499939|\n",
      "|2000-01-13|119.25| 120.0| 117.5|118.55| 3717.0|     USD| 0.69999695|  1.449997|    true|0.69999695|\n",
      "|2000-01-14|117.75|120.25|112.25|112.55|10115.0|     USD|   5.199997|  7.699997|    true|  5.199997|\n",
      "|2000-01-18|111.75|118.25| 110.6|115.75| 7364.0|     USD|       -4.0|       2.5|    true|       4.0|\n",
      "|2000-01-19| 116.5|118.25|114.75| 116.7| 6626.0|     USD|-0.19999695|  1.550003|    true|0.19999695|\n",
      "|2000-01-20|118.25| 118.8| 111.7| 112.0| 8834.0|     USD|       6.25|  6.800003|    true|      6.25|\n",
      "|2000-01-21| 112.0| 113.5| 110.8| 111.2| 5625.0|     USD| 0.80000305|  2.300003|    true|0.80000305|\n",
      "|2000-01-24|110.95| 114.4|110.95| 111.9| 5821.0|     USD| -0.9500046|       2.5|    true| 0.9500046|\n",
      "|2000-01-25| 111.6| 113.7| 111.6|112.85| 4014.0|     USD|      -1.25| 0.8499985|    true|      1.25|\n",
      "|2000-01-26| 112.5| 115.3| 111.9|115.15| 5796.0|     USD| -2.6500015|0.15000153|    true| 2.6500015|\n",
      "|2000-01-27|114.75| 116.4| 112.8| 114.6| 5477.0|     USD| 0.15000153|  1.800003|    true|0.15000153|\n",
      "|2000-01-28| 115.1| 115.4| 113.7| 114.7| 3334.0|     USD| 0.40000153| 0.7000046|    true|0.40000153|\n",
      "|2000-01-31|113.75| 114.0| 110.5| 111.1| 6465.0|     USD|  2.6500015| 2.9000015|    true| 2.6500015|\n",
      "+----------+------+------+------+------+-------+--------+-----------+----------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to the DataFrame where the values are the difference between 'Open' and 'Close'.\n",
    "from pyspark.sql.functions import *\n",
    "sdf = sdf.withColumn('open2close',\n",
    "                (sdf.open - sdf.close))\n",
    "# Add a column to the DataFrame where the values are the difference between 'High' and 'Low'.\n",
    "sdf = sdf.withColumn('high2low',\n",
    "                (sdf.high - sdf.close))\n",
    "# Add a column to the DataFrame where the values are 'True' if the volume for that day was 100 or above, and otherwise 'False'.\n",
    "sdf = sdf.withColumn('did_well',\n",
    "                     when((sdf.volume > 99), lit(True))\n",
    "                     .otherwise(lit(False)))\n",
    "# add another column that contains the absolute values of the numbers in that column.\n",
    "sdf = sdf.withColumn('day_abs', abs(sdf.open2close))\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sales = mean(sdf.open + sdf.high + sdf.low + sdf.close) * sdf.volume\n",
    "# print(net_sales)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd7cd7597f4635cf9895bddc64f3e7e6e4fea43a0bbbb31f8fc081c6ceb2aeb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
